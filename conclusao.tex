\newpage
\clearpage
\section{Considerações Finais}
\label{final:final}

Dentre as segmentações disponíveis e estudadas no decorrer desses estudos, entre as segmentações tradicionais (Seção \ref{segment:segment}), segmentações semânticas (Seção \ref{semantic:semantic}), segmentações de instancias (Seção \ref{instance:instance}) e segmentações panópticas (Seção \ref{panoptic:panoptic}) ficou evidente que a mais próxima do olhar e compreensão humana, que é atingido desde a infância \cite{Mohan2020}, é a segmentação panóptica, visto que classifica todos os pixeis de uma cena, além de proporcionar a separação de instâncias, podendo ter uma distinção dentre os objetos de mesma classe.

Dessa forma, nas tabelas \ref{conclusion:table:1} e \ref{conclusion:table:2}, é possível observar e comparar os métodos disponíveis para o trabalho com segmentações panópticas, destacando que os métodos foram realizadas nas bases de dados COCO \cite{Lin2014} e Cityscape \cite{Cordts2016}, sendo estas as principais bases de dados, como relatado na Seção \ref{panoptic:dataset}. Em adicional a essas, foi realizada uma terceira tabela (\ref{conclusion:table:3}) que conta com trabalhos realizados na base Mapillary Vistas \cite{Neuhold2017_ICCV}, a fim de que seja possível demonstrar a maior quantidade de trabalhos, redes e métodos já disponíveis.

Ainda quanto as tabelas \ref{conclusion:table:1}, \ref{conclusion:table:2} e \ref{conclusion:table:3}, vale citar que as mesmas são compostas pelos métodos desenvolvidos e sua referência, as redes bases, as métricas de qualidade $PQ$ e $PC$, expressas nas equações \ref{panoptic:eq:1} e \ref{panoptic:eq:3}, respectivamente, $RQ$ e $SQ$, assim como a aplicação da métrica de $PQ$ para \textit{things} e \textit{stuffs}.

\begin{table}[H]
    \centering
    \caption{Comparação de desempenho no \textit{dataset} COCO.}
    \label{conclusion:table:1}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|l|l|l|l|l|l|l}
    \textbf{Métodos}                                  &  \textbf{Rede Base}                          &  \textbf{PQ} &  \textbf{PQ-\textit{Thing}}    &    \textbf{PQ-\textit{Stuff}} &  \textbf{SQ} &  \textbf{RQ} &  \textbf{PC}  \\ \hline
    SOGNet\cite{Yang2020SOGNet:Segmentation}          &  ResNet-50  \cite{He2016}                            &   $43.7$     &  $50.6$                        &    $33.2$                     &  $78.7$      &   $53.5$     &   -       \\
    UPSNet\cite{Xiong2019}                            &  ResNet-50  \cite{He2016}                            &   $42.5$     &  $48.6$                        &    $33.4$                     &  -           &   -          &   -       \\
    OANet\cite{Liu2019}                               &  ResNet-101 \cite{He2016}                            &   $41.3$     &  $50.4$                        &    $27.7$                     &  -           &   -          &   -       \\
    OCFusion\cite{Lazarow2020LearningSegmentation}    &  ResNet-50  \cite{He2016}                            &   $41.0$     &  $49.0$                        &    $29.0$                     &  $77.1$      &   $50.6$     &   -       \\
    Panoptic FPN\cite{Kirillov2019}                   &  ResNet-101 \cite{He2016}                            &   $40.9$     &  $48.3$                        &    $29.7$                     &  -           &   -          &   -       \\
    AUNet\cite{Li2019Attention-guidedSegmentation}    &  ResNet-50  \cite{He2016}                            &   $39.6$     &  $49.1$                        &    $25.2$                     &  -           &   -          &   -       \\
    AdaptIS\cite{Sofiiuk2019AdaptIS:Network}          &  ResNet-101 \cite{He2016}                            &   $37.0$     &  $41.8$                        &    $29.9$                     &  -           &   -          &   -       \\
    DeeperLab\cite{Yang2019DeeperLab:Parser}          &  Xception-71 \cite{Chollet2017Xception:Convolutions} &   $34.3$     &  $37.5$                        &    $29.6$                     &  $77.1$      &   $43.1$     &   $56.8$ 
    \end{tabular}}

    \vspace*{1 cm}
    Fonte: retirado e adaptado de \cite{Awesome-panoptic-segmentation:List}.
\end{table}

\begin{table}[H]
    \centering
    \caption{Comparação de desemprenho no \textit{dataset} Cityscape.}
    \label{conclusion:table:2}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|l|l|l|l|l|l|l}
    \textbf{Métodos}                                &    \textbf{Rede Base}                         &  \textbf{PQ}        &  \textbf{PQ-\textit{Thing}}  &  \textbf{PQ-\textit{Stuff}}  & \textbf{SQ}  &  \textbf{RQ} &  \textbf{PC} \\ \hline
    Panoptic(Merge)                                 &  -                                                    &  $61.2$             &  $66.4$                      &  $54.0$                      &  $80.9$      &  $74.4$      &  -           \\
    AdaptIS\cite{Sofiiuk2019AdaptIS:Network}        &  ResNet-101 \cite{He2016}                             &  $60.6$             &  $58.7$                      &  $64.4$                      &  -           &  -           &  -           \\
    SOGNet\cite{Yang2020SOGNet:Segmentation}        &  ResNet-50  \cite{He2016}                             &  $60.0$             &  $56.7$                      &  $62.5$                      &  -           &  -           &  -           \\
    Seamless\cite{Porzi2019SeamlessSegmentation}    &  ResNet-50  \cite{He2016}                             &  $59.8$             &  $53.4$                      &  $64.5$                      &  -           &  -           &  -           \\
    UPSNet\cite{Xiong2019}                          &  ResNet-50  \cite{He2016}                             &  $59.3$             &  $54.6$                      &  $62.7$                      &  $79.7$      &  $73.0$      &  -           \\
    TASCNet\cite{Lazarow2020LearningSegmentation}   &  ResNet-101 \cite{He2016}                             &  $59.2$             &  $56  $                      &  $61.5$                      &  -           &  -           &  -           \\
    AUNet\cite{Li2019Attention-guidedSegmentation}  &  ResNet-101 \cite{He2016}                             &  $59.0$             &  $54.8$                      &  $62.1$                      &  -           &  -           &  -           \\
    Panoptic FPN\cite{Kirillov2019}                 &  ResNet-101 \cite{He2016}                             &  $58.1$             &  $52.0$                      &  $62.5$                      &  -           &  -           &  -           \\
    DeeperLab\cite{Yang2019DeeperLab:Parser}        &  Xception-71\cite{Chollet2017Xception:Convolutions}   &  $56.5$             &  -                           &  -                           &  -           &  -           &  $75.6$      
    \end{tabular}}

    \vspace*{1 cm}
    Fonte: retirado e adaptado de \cite{Awesome-panoptic-segmentation:List}.
\end{table}

\begin{table}[H]
    \centering
    \caption{Comparação de desempenho no \textit{dataset} Mapillary.}
    \label{conclusion:table:3}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|l|l|l|l|l|l|l}
    \textbf{Métodos}                                     & \textbf{Rede Base}                          & \textbf{PQ}  & \textbf{PQ-\textit{Thing}} & \textbf{PQ-\textit{Stuff}} &    \textbf{SQ}    &    \textbf{RQ} &    \textbf{PC} \\ \hline
    Panoptic(Merge)                                      & -                                                   & $38.3$       & $41.8$                     & $35.7$                     & $73.6$            & $47.7$         & -              \\
    Seamless\cite{Porzi2019SeamlessSegmentation}         & ResNet-50  \cite{He2016}                            & $37.2$       & $33.2$                     & $42.5$                     & -                 & -              & -              \\
    AdaptIS\cite{Sofiiuk2019AdaptIS:Network}             & ResNet-101 \cite{He2016}                            & $33.4$       & $28.3$                     & $40.3$                     & -                 & -              & -              \\
    TASCNetTASCNet\cite{Lazarow2020LearningSegmentation} & ResNet-101 \cite{He2016}                            & $32.6$       & $31.3$                     & $34.4$                     & -                 & -              & -              \\
    DeeperLab\cite{Yang2019DeeperLab:Parser}             & Xception-71 \cite{Chollet2017Xception:Convolutions} & $32.0$       & -                          & -                          & -                 & -              & $55.3$    
    \end{tabular}}

    \vspace*{1 cm}
    Fonte: retirado e adaptado de \cite{Awesome-panoptic-segmentation:List}.
\end{table}

A partir das tabelas possível observar que a maioria dos métodos desenvolvidos trabalha com a ResNet-50 ou ResNet-101 \cite{He2016}, com exceção do método DeeperLab \cite{Yang2019DeeperLab:Parser}, que também se destaca por ser a única que trabalha com a métrica $PC$ (equação \ref{panoptic:eq:3}).

Outrossim, vale observar que o valor de $PQ$ (equação \ref{panoptic:eq:1}) em qualquer uma das três tabelas não é superior a $61.2$, independente do método adotado, bem como é visível que o valor de $PQ-\textit{Thing}$ não é superior a  $66.4$ independente da base ou método, mas é superior à métrica de $PQ-\textit{Stuff}$, o que possui relação com as anotações inconsistentes realizadas nos \textit{datasets}, como é exposto por \cite{Kirillov2019a}.

Por fim, propostas que complementem as dificuldades citadas na Seção \ref{panoptic:conclusion} podem ser exploradas com o uso de técnicas de \textit{deep learning}, técnicas artesanais de processamento digitais de imagem (como os citados por \cite{pedrini2008analise}), tal como a unificação de modelos e arquiteturas (como proposto por \cite{Liu2019}) citadas nesse trabalho, de modo que às métricas de $PQ$ e $PC$ sejam aumentadas entre os \textit{datasets}.