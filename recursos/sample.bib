@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2016 - Deep residual learning for image recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {770--778},
publisher = {IEEE Computer Society},
title = {{Deep residual learning for image recognition}},
url = {http://image-net.org/challenges/LSVRC/2015/},
volume = {2016-December},
year = {2016}
}
@misc{AngLi,
title = {{Angzz/awesome-panoptic-segmentation: Panoptic Segmentation Resources List}},
url = {https://github.com/Angzz/awesome-panoptic-segmentation},
urldate = {2021-06-15}
}
@techreport{Parkhi2015,
abstract = {The goal of this paper is face recognition-from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M images , over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.},
author = {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parkhi, Vedaldi, Zisserman - 2015 - Deep Face Recognition.pdf:pdf},
keywords = {sbtmr},
publisher = {British Machine Vision Association},
title = {{Deep Face Recognition}},
year = {2015}
}
@article{Skarbek1994,
author = {Skarbek, Wladyslaw and Skarbek, Wladyslaw and Koschan, Andreas and Bericht, Technischer and Veroffentlichung, Zur and Klette, Prof Dr.},
title = {{Colour Image Segmentation - A Survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.1325},
year = {1994}
}
@inproceedings{Hariharan2014,
abstract = {We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top-down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16\% relative) over our baselines on SDS, a 5 point boost (10\% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1407.1808},
author = {Hariharan, Bharath and Arbel{\'{a}}ez, Pablo and Girshick, Ross and Malik, Jitendra},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10584-0_20},
eprint = {1407.1808},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hariharan et al. - 2014 - Simultaneous detection and segmentation.pdf:pdf},
isbn = {9783319105833},
issn = {16113349},
keywords = {convolutional networks,detection,segmentation},
month = {jul},
number = {PART 7},
pages = {297--312},
publisher = {Springer Verlag},
title = {{Simultaneous detection and segmentation}},
url = {http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds.},
volume = {8695 LNCS},
year = {2014}
}
@article{Martin2004,
abstract = {The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images.},
author = {Martin, David R. and Fowlkes, Charless C. and Malik, Jitendra},
doi = {10.1109/TPAMI.2004.1273918},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boundary detection,Boundary localization,Cue combination,Ground truth segmentation data set,Natural images,Supervised learning,Texture},
month = {may},
number = {5},
pages = {530--549},
pmid = {15460277},
title = {{Learning to detect natural image boundaries using local brightness, color, and texture cues}},
volume = {26},
year = {2004}
}
@inproceedings{Neubeck2006,
abstract = {In this work we scrutinize a low level computer vision task - non-maximum suppression (NMS) - which is a crucial preprocessing step in many computer vision applications. Especially in real time scenarios, efficient algorithms for such preprocessing algorithms, which operate on the full image resolution, are important. In the case of NMS, it seems that merely the straightforward implementation or slight improvements are known. We show that these are far from being optimal, and derive several algorithms ranging from easy-to-implement to highly-efficient.},
author = {Neubeck, Alexander and {Van Gool}, Luc},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2006.479},
isbn = {0769525210},
issn = {10514651},
pages = {850--855},
title = {{Efficient non-maximum suppression}},
volume = {3},
year = {2006}
}
@article{Sun2014,
abstract = {In the last few years, substantially different approaches have been adopted for segmenting and detecting 'things' (object categories that have a well defined shape such as people and cars) and 'stuff' (object categories which have an amorphous spatial extent such as grass and sky). While things have been typically detected by sliding window or Hough transform based methods, detection of stuff is generally formulated as a pixel or segment-wise classification problem. This paper proposes a framework for scene understanding that models both things and stuff using a common representation while preserving their distinct nature by using a property list. This representation allows us to enforce sophisticated geometric and semantic relationships between thing and stuff categories via property interactions in a single graphical model. We use the latest advances made in the field of discrete optimization to efficiently perform maximum a posteriori (MAP) inference in this model. We evaluate our method on the Stanford dataset by comparing it against state-of-the-art methods for object segmentation and detection. We also show that our method achieves competitive performances on the challenging PASCAL '09 segmentation dataset. {\textcopyright} 2014 IEEE.},
author = {Sun, Min and Kim, Byung Soo and Kohli, Pushmeet and Savarese, Silvio},
doi = {10.1109/TPAMI.2013.193},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Scene understanding,graph-cut,segmentation,semantic labeling},
month = {jul},
number = {7},
pages = {1370--1383},
publisher = {IEEE Computer Society},
title = {{Relating things and stuff via objectproperty interactions}},
url = {https://dl.acm.org/doi/abs/10.1109/TPAMI.2013.193},
volume = {36},
year = {2014}
}
@inproceedings{Yao2012,
abstract = {In this paper we propose an approach to holistic scene understanding that reasons jointly about regions, location, class and spatial extent of objects, presence of a class in the image, as well as the scene type. Learning and inference in our model are efficient as we reason at the segment level, and introduce auxiliary variables that allow us to decompose the inherent high-order potentials into pairwise potentials between a few variables with small number of states (at most the number of classes). Inference is done via a convergent message-passing algorithm, which, unlike graph-cuts inference, has no submodularity restrictions and does not require potential specific moves. We believe this is very important, as it allows us to encode our ideas and prior knowledge about the problem without the need to change the inference engine every time we introduce a new potential. Our approach outperforms the state-of-the-art on the MSRC-21 benchmark, while being much faster. Importantly, our holistic model is able to improve performance in all tasks. {\textcopyright} 2012 IEEE.},
author = {Yao, Jian and Fidler, Sanja and Urtasun, Raquel},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247739},
isbn = {9781467312264},
issn = {10636919},
pages = {702--709},
title = {{Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation}},
year = {2012}
}
@article{Varma2018,
abstract = {While several datasets for autonomous navigation have become available in recent years, they tend to focus on structured driving environments. This usually corresponds to well-delineated infrastructure such as lanes, a small number of well-defined categories for traffic participants, low variation in object or background appearance and strict adherence to traffic rules. We propose IDD, a novel dataset for road scene understanding in unstructured environments where the above assumptions are largely not satisfied. It consists of 10,004 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads. The label set is expanded in comparison to popular benchmarks such as Cityscapes, to account for new classes. It also reflects label distributions of road scenes significantly different from existing datasets, with most classes displaying greater within-class diversity. Consistent with real driving behaviours, it also identifies new classes such as drivable areas besides the road. We propose a new four-level label hierarchy, which allows varying degrees of complexity and opens up possibilities for new training methods. Our empirical study provides an in-depth analysis of the label characteristics. State-of-the-art methods for semantic segmentation achieve much lower accuracies on our dataset, demonstrating its distinction compared to Cityscapes. Finally, we propose that our dataset is an ideal opportunity for new problems such as domain adaptation, few-shot learning and behaviour prediction in road scenes.},
archivePrefix = {arXiv},
arxivId = {1811.10200},
author = {Varma, Girish and Subramanian, Anbumani and Namboodiri, Anoop and Chandraker, Manmohan and Jawahar, C V},
eprint = {1811.10200},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Varma et al. - 2018 - IDD A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments.pdf:pdf},
journal = {Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019},
month = {nov},
pages = {1743--1751},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{IDD: A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments}},
url = {http://arxiv.org/abs/1811.10200},
year = {2018}
}
@article{Geiger2013,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide. {\textcopyright} The Author(s) 2013.},
author = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
doi = {10.1177/0278364913491297},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geiger et al. - 2013 - Vision meets robotics The KITTI dataset.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Dataset,GPS,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
month = {sep},
number = {11},
pages = {1231--1237},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Vision meets robotics: The KITTI dataset}},
url = {http://www.cvlibs.net/datasets/kitti.},
volume = {32},
year = {2013}
}
@article{Zhou2016,
abstract = {Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A generic network design called Cascade Segmentation Module is then proposed to enable the segmentation networks to parse a scene into stuff, objects, and object parts in a cascade. We evaluate the proposed module integrated within two existing semantic segmentation networks, yielding significant improvements for scene parsing. We further show that the scene parsing networks trained on ADE20K can be applied to a wide variety of scenes and objects.},
archivePrefix = {arXiv},
arxivId = {1608.05442},
author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
eprint = {1608.05442},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2016 - Semantic Understanding of Scenes through the ADE20K Dataset.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {Deep neural networks,Image dataset,Instance segmentation,Scene understanding,Semantic segmentation},
month = {aug},
number = {3},
pages = {302--321},
publisher = {Springer New York LLC},
title = {{Semantic Understanding of Scenes through the ADE20K Dataset}},
url = {http://arxiv.org/abs/1608.05442},
volume = {127},
year = {2016}
}
@inproceedings{Neuhold2017_ICCV,
author = {Neuhold, Gerhard and Ollmann, Tobias and {Rota Bulo}, Samuel and Kontschieder, Peter},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
title = {{The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes}},
year = {2017}
}
@article{He2020,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
doi = {10.1109/TPAMI.2018.2844175},
eprint = {1703.06870},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2020 - Mask R-CNN.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Instance segmentation,convolutional neural network,object detection,pose estimation},
month = {feb},
number = {2},
pages = {386--397},
pmid = {29994331},
publisher = {IEEE Computer Society},
title = {{Mask R-CNN}},
url = {https://github.com/},
volume = {42},
year = {2020}
}
@article{Cordts2016,
abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
archivePrefix = {arXiv},
arxivId = {1604.01685},
author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
eprint = {1604.01685},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cordts et al. - 2016 - The Cityscapes Dataset for Semantic Urban Scene Understanding.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {apr},
pages = {3213--3223},
publisher = {IEEE Computer Society},
title = {{The Cityscapes Dataset for Semantic Urban Scene Understanding}},
url = {http://arxiv.org/abs/1604.01685},
volume = {2016-Decem},
year = {2016}
}
@article{Caesar2016,
abstract = {Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things.},
archivePrefix = {arXiv},
arxivId = {1612.03716},
author = {Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
eprint = {1612.03716},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caesar, Uijlings, Ferrari - 2016 - COCO-Stuff Thing and Stuff Classes in Context.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {dec},
pages = {1209--1218},
publisher = {IEEE Computer Society},
title = {{COCO-Stuff: Thing and Stuff Classes in Context}},
url = {http://arxiv.org/abs/1612.03716},
year = {2016}
}
@inproceedings{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2014 - Microsoft COCO Common objects in context.pdf:pdf},
issn = {16113349},
number = {PART 5},
pages = {740--755},
publisher = {Springer Verlag},
title = {{Microsoft COCO: Common objects in context}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48},
volume = {8693 LNCS},
year = {2014}
}
@article{Freund1997,
abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone-Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in ℝn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line. {\textcopyright} 1997 Academic Press.},
author = {Freund, Yoav and Schapire, Robert E.},
doi = {10.1006/jcss.1997.1504},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Freund, Schapire - 1997 - A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting.pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
number = {1},
pages = {119--139},
publisher = {Academic Press Inc.},
title = {{A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting}},
volume = {55},
year = {1997}
}
@article{Vaillant1994,
abstract = {An original approach is presented for the localization of objects in an image which approach is neuronal and has two steps. In the first step, a rough localization is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localizing faces in images.},
author = {Vaillant, R.},
doi = {10.1049/ip-vis:19941301},
issn = {1350245X},
journal = {IEE Proceedings - Vision, Image, and Signal Processing},
month = {aug},
number = {4},
pages = {245},
publisher = {IEE},
title = {{Original approach for the localisation of objects in images}},
url = {https://digital-library.theiet.org/content/journals/10.1049/ip-vis_19941301},
volume = {141},
year = {1994}
}
@article{Lin2016,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1612.03144},
author = {Lin, Tsung-Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
eprint = {1612.03144},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2016 - Feature Pyramid Networks for Object Detection.pdf:pdf},
month = {dec},
title = {{Feature Pyramid Networks for Object Detection}},
url = {http://arxiv.org/abs/1612.03144},
year = {2016}
}
@misc{Kumar2019,
abstract = {In semantic segmentation, the goal is to classify each pixel into the given classes. In instance segmentation, we care about segmentation of the instances of objects separately. The panoptic segmentation combines semantic and instance segmentation such that all pixels are assigned a class label and all object instances are uniquely segmented.},
booktitle = {Technical Fridays},
pages = {1},
title = {{Introduction to Panoptic Segmentation: A Tutorial}},
url = {https://kharshit.github.io/blog/2019/10/18/introduction-to-panoptic-segmentation-tutorial},
urldate = {2021-06-01},
year = {2019}
}
@article{Bolya2019,
abstract = {We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.},
archivePrefix = {arXiv},
arxivId = {1904.02689},
author = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae},
eprint = {1904.02689},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bolya et al. - 2019 - YOLACT Real-time Instance Segmentation.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
month = {apr},
pages = {9156--9165},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{YOLACT: Real-time Instance Segmentation}},
url = {http://arxiv.org/abs/1904.02689},
volume = {2019-Octob},
year = {2019}
}
@article{Hesamian2019,
abstract = {Deep learning-based image segmentation is by now firmly established as a robust tool in image segmentation. It has been widely used to separate homogeneous areas as the first and critical component of diagnosis and treatment pipeline. In this article, we present a critical appraisal of popular methods that have employed deep-learning techniques for medical image segmentation. Moreover, we summarize the most common challenges incurred and suggest possible solutions.},
author = {Hesamian, Mohammad Hesam and Jia, Wenjing and He, Xiangjian and Kennedy, Paul},
doi = {10.1007/s10278-019-00227-x},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hesamian et al. - 2019 - Deep Learning Techniques for Medical Image Segmentation Achievements and Challenges.pdf:pdf},
issn = {1618727X},
journal = {Journal of Digital Imaging},
keywords = {CNN,Deep learning,Medical image segmentation,Organ segmentation},
month = {aug},
number = {4},
pages = {582--596},
pmid = {31144149},
publisher = {Springer New York LLC},
title = {{Deep Learning Techniques for Medical Image Segmentation: Achievements and Challenges}},
url = {https://doi.org/10.1007/s10278-019-00227-x},
volume = {32},
year = {2019}
}
@book{Goodfellow2016,
author = {Goodfellow, I. and Bengio, Y. and Courville, A.},
booktitle = {MIT Press},
edition = {2},
editor = {{Cambridge: MIT press}},
title = {{Deep Learning}},
url = {https://synapse.koreamed.org/upload/SynapseData/PDFData/1088HIR/hir-22-351.pdf},
volume = {1},
year = {2016}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychological Review},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATI},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
url = {https://psycnet.apa.org/journals/rev/65/6/386},
volume = {65},
year = {1958}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McCulloch, Pitts - 1943 - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
keywords = {Cell Biology,Life Sciences,Mathematical and Computational Biology,general},
month = {dec},
number = {4},
pages = {115--133},
publisher = {Kluwer Academic Publishers},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://link.springer.com/article/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@techreport{Noh,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of de-convolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366v1},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
eprint = {1505.04366v1},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Noh, Hong, Han - Unknown - Learning Deconvolution Network for Semantic Segmentation.pdf:pdf},
title = {{Learning Deconvolution Network for Semantic Segmentation}}
}
@inproceedings{Arbelaez2012,
abstract = {We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects. {\textcopyright} 2012 IEEE.},
author = {Arbelaez, Pablo and Hariharan, Bharath and Gu, Chunhui and Gupta, Saurabh and Bourdev, Lubomir and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6248077},
isbn = {9781467312264},
issn = {10636919},
pages = {3378--3385},
title = {{Semantic segmentation using regions and parts}},
year = {2012}
}
@article{Yu2018,
abstract = {Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4\% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.},
archivePrefix = {arXiv},
arxivId = {1808.00897},
author = {Yu, Changqian and Wang, Jingbo and Peng, Chao and Gao, Changxin and Yu, Gang and Sang, Nong},
eprint = {1808.00897},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2018 - BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bilateral,Network,Real-time,Segmentation,Segmentation {\textperiodcentered},Semantic},
month = {aug},
pages = {334--349},
publisher = {Springer Verlag},
title = {{BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation}},
url = {http://arxiv.org/abs/1808.00897},
volume = {11217 LNCS},
year = {2018}
}
@article{Zhang2018,
abstract = {Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7\% mIoU on PASCAL-Context, 85.9\% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45\%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.},
archivePrefix = {arXiv},
arxivId = {1803.08904},
author = {Zhang, Hang and Dana, Kristin and Shi, Jianping and Zhang, Zhongyue and Wang, Xiaogang and Tyagi, Ambrish and Agrawal, Amit},
eprint = {1803.08904},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2018 - Context Encoding for Semantic Segmentation.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {mar},
pages = {7151--7160},
publisher = {IEEE Computer Society},
title = {{Context Encoding for Semantic Segmentation}},
url = {http://arxiv.org/abs/1803.08904},
year = {2018}
}
@techreport{Csurka,
abstract = {In this work, we consider the evaluation of the semantic segmentation task. We discuss the strengths and limitations of the few existing measures, and propose new ways to evaluate semantic segmentation. First, we argue that a per-image score instead of one computed over the entire dataset brings a lot more insight. Second, we propose to take contours more carefully into account. Based on the conducted experiments, we suggest best practices for the evaluation. Finally, we present a user study we conducted to better understand how the quality of image segmentations is perceived by humans.},
author = {Csurka, Gabriela and Larlus, Diane and Perronnin, Florent},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Csurka, Larlus, Perronnin - Unknown - CSURKA, LARLUS, PERRONNIN EVALUATION OF SEMANTIC SEGMENTATION What is a good evaluation measure fo.pdf:pdf},
publisher = {British Machine Vision Conference},
title = {{What is a good evaluation measure for semantic segmentation?}}
}
@inproceedings{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 - achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/$\sim$rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
month = {sep},
pages = {580--587},
publisher = {IEEE Computer Society},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://www.cs.berkeley.edu/˜rbg/rcnn.},
year = {2014}
}
@article{Shelhamer2016,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1605.06211},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
eprint = {1605.06211},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shelhamer, Long, Darrell - 2016 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
keywords = {Convolutional Networks,Deep Learning,Index Terms-Semantic Segmentation,Transfer Learning !},
month = {may},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://arxiv.org/abs/1605.06211},
year = {2016}
}
@article{Kirillov2019,
abstract = {The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.},
archivePrefix = {arXiv},
arxivId = {1901.02446},
author = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
eprint = {1901.02446},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirillov et al. - 2019 - Panoptic Feature Pyramid Networks.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Grouping and Shape,Recognition: Detection,Retrieval,Segmentation},
month = {jan},
pages = {6392--6401},
publisher = {IEEE Computer Society},
title = {{Panoptic Feature Pyramid Networks}},
url = {http://arxiv.org/abs/1901.02446},
volume = {2019-June},
year = {2019}
}
@misc{Carneiro2021,
abstract = {Pest and disease control plays a key role in agriculture since the damage caused by these agents are responsible for a huge economic loss every year. Based on this assumption, we create an algorithm capable of detecting rust (Hemileia vastatrix) and leaf miner (Leucoptera coffeella) in coffee leaves (Coffea arabica) and quantify disease severity using a mobile application as a high-level interface for the model inferences. We used different convolutional neural network architectures to create the object detector, besides the OpenCV library, k-means, and three treatments: the RGB and value to quantification, and the AFSoft software, in addition to the analysis of variance, where we compare the three methods. The results show an average precision of 81,5\% in the detection and that there was no significant statistical difference between treatments to quantify the severity of coffee leaves, proposing a computationally less costly method. The application, together with the trained model, can detect the pest and disease over different image conditions and infection stages and also estimate the disease infection stage.},
author = {Carneiro, A.L.C. and {de Brito Silva}, L. and Faulin, M.A.R.},
booktitle = {arXiv},
issn = {23318422},
keywords = {Artificial intelligence,Convolutional neural networks,Deep learning,Object detection,Plant disease identification},
title = {{Artificial intelligence for detection and quantification of rust and leaf miner in coffee crop}},
year = {2021}
}
@inproceedings{Jaiswal2021,
abstract = {Along with computer technology, the demand of digital image processing is too high and it is used massively in every sector like organization, business, medical and so on. Image segmentation enables us to analyze any given image in order to extract information from the image. Numerous algorithm and techniques have been industrialized in the field of image segmentation. Segmentation has become one of the prominent tasks in machine vision. Machine vision enables the machine to vision the real-world problems like human does and also acts accordingly to solve the problem, so it is utmost important to come up with the techniques that can be applied for the image segmentations. Invention of modern segmentation methods like instance, semantic and panoptic segmentation has advanced the concept of machine vision. This paper focuses on the various methods of image segmentation along with its advantages and disadvantages.},
author = {Jaiswal, Sushma and Pandey, M. K.},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-981-15-6014-9_27},
isbn = {9789811560132},
issn = {21945365},
keywords = {Clustering,Edge detection,Fuzzy-c-means,GA,Histogram,Image segmentation,Machine vision,PCA,Region growing,SVM},
pages = {233--240},
title = {{A Review on Image Segmentation}},
volume = {1187},
year = {2021}
}
@article{Hafiz2020,
abstract = {Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation -- its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.},
archivePrefix = {arXiv},
arxivId = {2007.00047},
author = {Hafiz, Abdul Mueed and Bhat, Ghulam Mohiuddin},
doi = {10.1007/s13735-020-00195-x},
eprint = {2007.00047},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hafiz, Bhat - 2020 - A Survey on Instance Segmentation State of the art.pdf:pdf},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {Convolutional neural networks,Deep learning,Instance segmentation,Object detection},
month = {jun},
number = {3},
pages = {171--189},
publisher = {Springer},
title = {{A Survey on Instance Segmentation: State of the art}},
url = {http://arxiv.org/abs/2007.00047 http://dx.doi.org/10.1007/s13735-020-00195-x},
volume = {9},
year = {2020}
}
@article{Minaee2021,
abstract = {Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of Deep Learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.},
archivePrefix = {arXiv},
arxivId = {2001.05566},
author = {Minaee, Shervin and Boykov, Yuri Y. and Porikli, Fatih and Plaza, Antonio J. and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
doi = {10.1109/TPAMI.2021.3059968},
eprint = {2001.05566},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Minaee et al. - 2020 - Image Segmentation Using Deep Learning A Survey.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computational modeling,Computer architecture,Deep learning,Generative adversarial networks,Image segmentation,Logic gates,Semantics,convolutional neural networks,deep learning,encoder-decoder models,generative models,instance segmentation,medical image segmentation,panoptic segmentation,recurrent models,semantic segmentation},
month = {jan},
publisher = {IEEE Computer Society},
title = {{Image Segmentation Using Deep Learning: A Survey}},
url = {http://arxiv.org/abs/2001.05566},
year = {2021}
}
@article{Yanowitz1989,
abstract = {It is often required to separate objects from background in conditions of poor illumination in image processing. Adaptive methods can learn the illumination from the given images and base a decision on this information. Here the threshold surface is determined by interpolating the gray levels at points where the gradient is high, indicating probable object edges. -after Authors},
author = {Yanowitz, S. D. and Bruckstein, A. M.},
doi = {10.1016/S0734-189X(89)80017-9},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yanowitz, Bruckstein - 1989 - A new method for image segmentation.pdf:pdf},
issn = {0734189X},
journal = {Computer Vision, Graphics, & Image Processing},
month = {apr},
number = {1},
pages = {82--95},
publisher = {Academic Press},
title = {{A new method for image segmentation}},
volume = {46},
year = {1989}
}
@article{Pal1993,
abstract = {Many image segmentation techniques are available in the literature. Some of these techniques use only the gray level histogram, some use spatial details while others use fuzzy set theoretic approaches. Most of these techniques are not suitable for noisy environments. Some works have been done using the Markov Random Field (MRF) model which is robust to noise, but is computationally involved. Neural network architectures which help to get the output in real time because of their parallel processing ability, have also been used for segmentation and they work fine even when the noise level is very high. The literature on color image segmentation is not that rich as it is for gray tone images. This paper critically reviews and summarizes some of these techniques. Attempts have been made to cover both fuzzy and non-fuzzy techniques including color image segmentation and neural network based approaches. Adequate attention is paid to segmentation of range images and magnetic resonance images. It also addresses the issue of quantitative evaluation of segmentation results. {\textcopyright} 1993.},
author = {Pal, Nikhil R. and Pal, Sankar K.},
doi = {10.1016/0031-3203(93)90135-J},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pal, Pal - 1993 - A review on image segmentation techniques.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Clustering,Edge detection,Fuzzy sets,Image segmentation,Markov Random Field,Relaxation,Thresholding},
month = {sep},
number = {9},
pages = {1277--1294},
publisher = {Pergamon},
title = {{A review on image segmentation techniques}},
volume = {26},
year = {1993}
}
@article{Haralick1985,
abstract = {A wide variety of image segmentation techniques exist, some considered general purpose and some designed for specific classes of images. These techniques can be classified as: measurement space guided spatial clustering, single linkage region growing schemes, hybrid linkage region growing schemes, centroid linkage region growing schemes, spatial clustering schemes, and split-and-merge schemes. Each of the major classes of image segmentation techniques is defined and several specific examples of each class of algorithm are described. The techniques are illustrated with examples of segmentations performed on real images.-after Authors},
author = {Haralick, R. M. and Shapiro, L. G.},
doi = {10.1016/S0734-189X(85)90153-7},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haralick, Shapiro - 1985 - Image segmentation techniques.pdf:pdf},
issn = {0734189X},
journal = {Computer Vision, Graphics, & Image Processing},
month = {jan},
number = {1},
pages = {100--132},
publisher = {Academic Press},
title = {{Image segmentation techniques.}},
volume = {29},
year = {1985}
}
@article{Lai2015,
abstract = {This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer's Disease. We found that a slightly unconventional "stacked 2D" approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular "tri-planar" approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement.},
archivePrefix = {arXiv},
arxivId = {1505.02000},
author = {Lai, Matthew},
eprint = {1505.02000},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai - 2015 - Deep Learning for Medical Image Segmentation.pdf:pdf},
month = {may},
pages = {40--77},
title = {{Deep Learning for Medical Image Segmentation}},
url = {http://arxiv.org/abs/1505.02000},
year = {2015}
}
@article{Ghosh2019,
archivePrefix = {arXiv},
arxivId = {1907.06119},
author = {Ghosh, Swarnendu and Das, Nibaran and Das, Ishita and Maulik, Ujjwal},
doi = {10.1145/3329784},
eprint = {1907.06119},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghosh et al. - 2019 - Understanding Deep Learning Techniques for Image Segmentation.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Convolutional neural networks,Deep learning,Semantic image segmentation},
number = {4},
publisher = {Association for Computing Machinery},
title = {{Understanding Deep Learning Techniques for Image Segmentation}},
url = {http://arxiv.org/abs/1907.06119 https://doi.org/10.1145/3329784},
volume = {52},
year = {2019}
}
@inproceedings{Kirillov2019a,
abstract = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper: {smallurl{https://arxiv.org/abs/1801.00868}.}},
author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00963},
file = {:home/brito/Documentos/Mestrado/Dissertacao/1801.00868.pdf:pdf},
issn = {10636919},
title = {{Panoptic segmentation}},
volume = {2019-June},
year = {2019}
}
@article{Kim2020,
abstract = {Panoptic segmentation has become a new standard of visual recognition task by unifying previous semantic segmentation and instance segmentation tasks in concert. In this paper, we propose and explore a new video extension of this task, called video panoptic segmentation. The task requires generating consistent panoptic segmentation as well as an association of instance ids across video frames. To invigorate research on this new task, we present two types of video panoptic datasets. The first is a re-organization of the synthetic VIPER dataset into the video panoptic format to exploit its large-scale pixel annotations. The second is a temporal extension on the Cityscapes val. set, by providing new video panoptic annotations (Cityscapes-VPS). Moreover, we propose a novel video panoptic segmentation network (VPSNet) which jointly predicts object classes, bounding boxes, masks, instance id tracking, and semantic segmentation in video frames. To provide appropriate metrics for this task, we propose a video panoptic quality (VPQ) metric and evaluate our method and several other baselines. Experimental results demonstrate the effectiveness of the presented two datasets. We achieve state-of-the-art results in image PQ on Cityscapes and also in VPQ on Cityscapes-VPS and VIPER datasets. The datasets and code are made publicly available.},
archivePrefix = {arXiv},
arxivId = {2006.11339},
author = {Kim, Dahun and Woo, Sanghyun and Lee, Joon-Young and Kweon, In So},
eprint = {2006.11339},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2020 - Video Panoptic Segmentation.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {9856--9865},
publisher = {IEEE Computer Society},
title = {{Video Panoptic Segmentation}},
url = {http://arxiv.org/abs/2006.11339},
year = {2020}
}
@article{DeGeus2019a,
abstract = {In this work, we present an end-to-end network for fast panoptic segmentation. This network, called Fast Panoptic Segmentation Network (FPSNet), does not require computationally costly instance mask predictions or merging heuristics. This is achieved by casting the panoptic task into a custom dense pixel-wise classification task, which assigns a class label or an instance id to each pixel. We evaluate FPSNet on the Cityscapes and Pascal VOC datasets, and find that FPSNet is faster than existing panoptic segmentation methods, while achieving better or similar panoptic segmentation performance. On the Cityscapes validation set, we achieve a Panoptic Quality score of 55.1\%, at prediction times of 114 milliseconds for images with a resolution of 1024x2048 pixels. For lower resolutions of the Cityscapes dataset and for the Pascal VOC dataset, FPSNet runs at 22 and 35 frames per second, respectively.},
archivePrefix = {arXiv},
arxivId = {1910.03892},
author = {{De Geus}, Daan and Meletis, Panagiotis and Dubbelman, Gijs},
doi = {10.1109/LRA.2020.2969919},
eprint = {1910.03892},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Geus, Meletis, Dubbelman - 2019 - Fast Panoptic Segmentation Network.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Semantic scene understanding,deep learning in robotics and automation,object detection,segmentation and categorization},
month = {oct},
number = {2},
pages = {1742--1749},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Fast Panoptic Segmentation Network}},
url = {http://arxiv.org/abs/1910.03892},
volume = {5},
year = {2019}
}
@article{Xiong2019,
abstract = {In this paper, we propose a unified panoptic segmentation network (UPSNet) for tackling the newly proposed panoptic segmentation task. On top of a single backbone residual network, we first design a deformable convolution based semantic segmentation head and a Mask R-CNN style instance segmentation head which solve these two subtasks simultaneously. More importantly, we introduce a parameter-free panoptic head which solves the panoptic segmentation via pixel-wise classification. It first leverages the logits from the previous two heads and then innovatively expands the representation for enabling prediction of an extra unknown class which helps better resolve the conflicts between semantic and instance segmentation. Additionally, it handles the challenge caused by the varying number of instances and permits back propagation to the bottom modules in an end-to-end manner. Extensive experimental results on Cityscapes, COCO and our internal dataset demonstrate that our UPSNet achieves state-of-the-art performance with much faster inference. Code has been made available at: https://github.com/uber-research/UPSNet},
archivePrefix = {arXiv},
arxivId = {1901.03784},
author = {Xiong, Yuwen and Liao, Renjie and Zhao, Hengshuang and Hu, Rui and Bai, Min and Yumer, Ersin and Urtasun, Raquel},
eprint = {1901.03784},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong et al. - 2019 - UPSNet A Unified Panoptic Segmentation Network.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Deep Learning,Grouping and Shape,Recognition: Detection,Retrieval,Scene Analysis and Understanding,Segmentation},
month = {jan},
pages = {8810--8818},
publisher = {IEEE Computer Society},
title = {{UPSNet: A Unified Panoptic Segmentation Network}},
url = {http://arxiv.org/abs/1901.03784},
volume = {2019-June},
year = {2019}
}
@article{Mohan2020,
abstract = {Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date.},
archivePrefix = {arXiv},
arxivId = {2004.02307},
author = {Mohan, Rohit and Valada, Abhinav},
eprint = {2004.02307},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohan, Valada - 2020 - EfficientPS Efficient Panoptic Segmentation.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {Instance segmentation,Panoptic segmentation,Scene understanding,Semantic segmentation},
month = {apr},
number = {5},
pages = {1551--1579},
publisher = {Springer},
title = {{EfficientPS: Efficient Panoptic Segmentation}},
url = {http://arxiv.org/abs/2004.02307},
volume = {129},
year = {2020}
}
@article{Chen2019,
abstract = {Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects: (1) instead of performing cascaded refinement on these two tasks separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a single HTC obtains 38.4 and 1.5 improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018 Challenge Object Detection Task. Code is available at: https://github.com/open-mmlab/mmdetection.},
archivePrefix = {arXiv},
arxivId = {1901.07518},
author = {Chen, Kai and Pang, Jiangmiao and Wang, Jiaqi and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
eprint = {1901.07518},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2019 - Hybrid Task Cascade for Instance Segmentation.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Recognition: Detection,Retrieval},
month = {jan},
pages = {4969--4978},
publisher = {IEEE Computer Society},
title = {{Hybrid Task Cascade for Instance Segmentation}},
url = {http://arxiv.org/abs/1901.07518},
volume = {2019-June},
year = {2019}
}
@article{Wang2017,
abstract = {Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields (RF) of the network to aggregate global information; 2) alleviates what we call the "gridding issue" caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1\% mIOU in the test set at the time of submission. We also have achieved state-of-the-art overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https://github.com/TuSimple/TuSimple-DUC .},
archivePrefix = {arXiv},
arxivId = {1702.08502},
author = {Wang, Panqu and Chen, Pengfei and Yuan, Ye and Liu, Ding and Huang, Zehua and Hou, Xiaodi and Cottrell, Garrison},
eprint = {1702.08502},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Understanding Convolution for Semantic Segmentation.pdf:pdf},
journal = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
month = {feb},
pages = {1451--1460},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Understanding Convolution for Semantic Segmentation}},
url = {http://arxiv.org/abs/1702.08502},
volume = {2018-Janua},
year = {2017}
}
@article{DeGeus2019,
abstract = {In this work, we propose a single deep neural network for panoptic segmentation, for which the goal is to provide each individual pixel of an input image with a class label, as in semantic segmentation, as well as a unique identifier for specific objects in an image, following instance segmentation. Our network makes joint semantic and instance segmentation predictions and combines these to form an output in the panoptic format. This has two main benefits: firstly, the entire panoptic prediction is made in one pass, reducing the required computation time and resources; secondly, by learning the tasks jointly, information is shared between the two tasks, thereby improving performance. Our network is evaluated on two street scene datasets: Cityscapes and Mapillary Vistas. By leveraging information exchange and improving the merging heuristics, we increase the performance of the single network, and achieve a score of 23.9 on the Panoptic Quality (PQ) metric on Mapillary Vistas validation, with an input resolution of 640 x 900 pixels. On Cityscapes validation, our method achieves a PQ score of 45.9 with an input resolution of 512 x 1024 pixels. Moreover, our method decreases the prediction time by a factor of 2 with respect to separate networks.},
archivePrefix = {arXiv},
arxivId = {1902.02678},
author = {de Geus, Daan and Meletis, Panagiotis and Dubbelman, Gijs},
eprint = {1902.02678},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Geus, Meletis, Dubbelman - 2019 - Single Network Panoptic Segmentation for Street Scene Understanding.pdf:pdf},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
month = {feb},
pages = {709--715},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Single Network Panoptic Segmentation for Street Scene Understanding}},
url = {http://arxiv.org/abs/1902.02678},
volume = {2019-June},
year = {2019}
}
@article{Chen2019a,
abstract = {Object location is fundamental to panoptic segmentation as it is related to all things and stuff in the image scene. Knowing the locations of objects in the image provides clues for segmenting and helps the network better understand the scene. How to integrate object location in both thing and stuff segmentation is a crucial problem. In this paper, we propose spatial information flows to achieve this objective. The flows can bridge all sub-tasks in panoptic segmentation by delivering the object's spatial context from the box regression task to others. More importantly, we design four parallel sub-networks to get a preferable adaptation of object spatial information in sub-tasks. Upon the sub-networks and the flows, we present a location-aware and unified framework for panoptic segmentation, denoted as SpatialFlow. We perform a detailed ablation study on each component and conduct extensive experiments to prove the effectiveness of SpatialFlow. Furthermore, we achieve state-of-the-art results, which are $47.9$ PQ and $62.5$ PQ respectively on MS-COCO and Cityscapes panoptic benchmarks. Code will be available at https://github.com/chensnathan/SpatialFlow.},
archivePrefix = {arXiv},
arxivId = {1910.08787},
author = {Chen, Qiang and Cheng, Anda and He, Xiangyu and Wang, Peisong and Cheng, Jian},
eprint = {1910.08787},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2019 - SpatialFlow Bridging All Tasks for Panoptic Segmentation.pdf:pdf},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Benchmark testing,Detectors,Head,Image segmentation,Location-aware,Object detection,Panoptic segmentation,Scene understanding,Semantics,Task analysis},
month = {oct},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{SpatialFlow: Bridging All Tasks for Panoptic Segmentation}},
url = {http://arxiv.org/abs/1910.08787},
year = {2019}
}
@article{Liu2019,
abstract = {Panoptic segmentation, which needs to assign a category label to each pixel and segment each object instance simultaneously, is a challenging topic. Traditionally, the existing approaches utilize two independent models without sharing features, which makes the pipeline inefficient to implement. In addition, a heuristic method is usually employed to merge the results. However, the overlapping relationship between object instances is difficult to determine without sufficient context information during the merging process. To address the problems, we propose a novel end-to-end network for panoptic segmentation, which can efficiently and effectively predict both the instance and stuff segmentation in a single network. Moreover, we introduce a novel spatial ranking module to deal with the occlusion problem between the predicted instances. Extensive experiments have been done to validate the performance of our proposed method and promising results have been achieved on the COCO Panoptic benchmark.},
archivePrefix = {arXiv},
arxivId = {1903.05027},
author = {Liu, Huanyu and Peng, Chao and Yu, Changqian and Wang, Jingbo and Liu, Xu and Yu, Gang and Jiang, Wei},
eprint = {1903.05027},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2019 - An End-to-End Network for Panoptic Segmentation.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Deep Learning,Recognition: Detection,Retrieval,Robotics + Driving,Scene Analysis and Understanding,Segmentation},
month = {mar},
pages = {6165--6174},
publisher = {IEEE Computer Society},
title = {{An End-to-End Network for Panoptic Segmentation}},
url = {http://arxiv.org/abs/1903.05027},
volume = {2019-June},
year = {2019}
}
@article{Hou2019,
abstract = {Panoptic segmentation is a complex full scene parsing task requiring simultaneous instance and semantic segmentation at high resolution. Current state-of-the-art approaches cannot run in real-time, and simplifying these architectures to improve efficiency severely degrades their accuracy. In this paper, we propose a new single-shot panoptic segmentation network that leverages dense detections and a global self-attention mechanism to operate in real-time with performance approaching the state of the art. We introduce a novel parameter-free mask construction method that substantially reduces computational complexity by efficiently reusing information from the object detection and semantic segmentation sub-tasks. The resulting network has a simple data flow that does not require feature map re-sampling or clustering post-processing, enabling significant hardware acceleration. Our experiments on the Cityscapes and COCO benchmarks show that our network works at 30 FPS on 1024x2048 resolution, trading a 3\% relative performance degradation from the current state of the art for up to 440\% faster inference.},
archivePrefix = {arXiv},
arxivId = {1912.01202},
author = {Hou, Rui and Li, Jie and Bhargava, Arjun and Raventos, Allan and Guizilini, Vitor and Fang, Chao and Lynch, Jerome and Gaidon, Adrien},
eprint = {1912.01202},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou et al. - 2019 - Real-Time Panoptic Segmentation from Dense Detections.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {dec},
pages = {8520--8529},
publisher = {IEEE Computer Society},
title = {{Real-Time Panoptic Segmentation from Dense Detections}},
url = {http://arxiv.org/abs/1912.01202},
year = {2019}
}
@inproceedings{Zhao2017,
abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
archivePrefix = {arXiv},
arxivId = {1612.01105},
author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.660},
eprint = {1612.01105},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2017 - Pyramid scene parsing network.pdf:pdf},
isbn = {9781538604571},
month = {nov},
pages = {6230--6239},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Pyramid scene parsing network}},
url = {https://github.com/hszhao/PSPNet},
volume = {2017-Janua},
year = {2017}
}
@article{Brunger2020,
abstract = {The behavioural research of pigs can be greatly simplified if automatic recognition systems are used. Especially systems based on computer vision have the advantage that they allow an evaluation without affecting the normal behaviour of the animals. In recent years, methods based on deep learning have been introduced and have shown pleasingly good results. Especially object and keypoint detectors have been used to detect the individual animals. Despite good results, bounding boxes and sparse keypoints do not trace the contours of the animals, resulting in a lot of information being lost. Therefore this work follows the relatively new definition of a panoptic segmentation and aims at the pixel accurate segmentation of the individual pigs. For this a framework of a neural network for semantic segmentation, different network heads and postprocessing methods is presented. With the resulting instance segmentation masks further information like the size or weight of the animals could be estimated. The method is tested on a specially created data set with 1000 hand-labeled images and achieves detection rates of around 95\% (F1 Score) despite disturbances such as occlusions and dirty lenses.},
archivePrefix = {arXiv},
arxivId = {2005.10499},
author = {Br{\"{u}}nger, Johannes and Gentz, Maria and Traulsen, Imke and Koch, Reinhard},
doi = {10.3390/s20133710},
eprint = {2005.10499},
file = {:home/brito/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Br{\"{u}}nger et al. - 2020 - Panoptic Instance Segmentation on Pigs.pdf:pdf},
journal = {Sensors (Switzerland)},
keywords = {Animal detection,Computer vision,Deep learning,Image processing,Pose estimation,Precision livestock},
month = {may},
number = {13},
pages = {1--21},
publisher = {MDPI AG},
title = {{Panoptic Instance Segmentation on Pigs}},
url = {http://arxiv.org/abs/2005.10499 http://dx.doi.org/10.3390/s20133710},
volume = {20},
year = {2020}
}
